# -*- coding: utf-8 -*-
"""F9Tuned.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GWtGFsTIuf1JDjvZGN9Uk0AAdhudasA0
"""

import warnings
import pandas as pd
import numpy as np
import joblib
import multiprocessing
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, StackingClassifier
from sklearn.dummy import DummyClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report

path = "online_shoppers_intention.csv"
df = pd.read_csv(path)

# Da kommen sonst seitenweise Warnungen (UserWarning, FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

# Zielvariable und Boolesche Spalte
df['Revenue'] = LabelEncoder().fit_transform(df['Revenue'])
df['Weekend'] = df['Weekend'].astype(int)

# Vorgegebene Features direkt auswählen, Recursive Factor Elimnation wurde davor durchgeführt und die Hierarchie der optimalen Features identifiziert. Dann wurde RFE mit CV angewendet, um die optimale anzahl an Features (optimiert zum F1 score) zu finden, was 9 ist
selected_features = [
    'Informational', 'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay',
    'Month', 'OperatingSystems', 'VisitorType', 'Weekend'
]

# Datensatz auf die ausgewählte Features verkleinern
X_selected = df[selected_features].copy()

# Kategorische Spalten im verkleinerten Datensatz identifizieren
categorical_cols = ['Month', 'VisitorType', 'Weekend']

# Kategorische Features mit One-Hot-Encoding umwandeln
if categorical_cols:
    X_selected = pd.get_dummies(X_selected, columns=categorical_cols, drop_first=True)

# Zielvariable
y = df['Revenue']

# Datensatz splitten (20% Testgröße, stratified)
X_train, X_test, y_train, y_test = train_test_split(
    X_selected, y, test_size=0.2, stratify=y, random_state=42
)

# SMOTE nur für Boosting-Modelle auf Trainingsdaten anwenden, SMOTE hilft, weil es sich um ein stark unbalancierten Datensatz handelt
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Anzahl CPU-Kerne für Parallelisierung ermitteln
cores = multiprocessing.cpu_count() - 1

# Baseline und einfache Modelle definieren
baseline = DummyClassifier(strategy="most_frequent")
logreg = Pipeline([
    ('scaler', StandardScaler()),
    ('clf', LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs'))
])
tree = DecisionTreeClassifier(random_state=42)

# Optimierte Random Forest Hyperparameter verwenden, wie bei der RFE fand das vorher statt. Um compute time zu sparen wurde ndie optimalen Werte eingesetzt und es wird nicht jedes mal getuned
best_rf = RandomForestClassifier(
    n_estimators=100,
    min_samples_leaf=5,
    max_samples=0.7,
    max_features=7,
    bootstrap=True,
    random_state=42,
    n_jobs=cores
)
best_rf.fit(X_train, y_train)

# Optimierte XGBoost Hyperparameter verwenden, auch hier wie beim best_rf schon davor gemacht
xgb_model = XGBClassifier(
    eval_metric='logloss',
    random_state=42,
    verbosity=0,
    use_label_encoder=False,
    n_estimators=300,
    learning_rate=0.05,
    max_depth=10,
    colsample_bytree=0.8,
    gamma=0.3,
    reg_alpha=0.5,
    reg_lambda=0.5,
    subsample=0.8
)
xgb_model.fit(X_train_resampled, y_train_resampled)

# Optimierte LightGBM Hyperparameter verwenden, wieder davor gemacht
lgbm_model = LGBMClassifier(
    class_weight='balanced',
    random_state=42,
    n_estimators=250,
    learning_rate=0.1,
    max_depth=10,
    num_leaves=72,
    min_child_samples=5,
    colsample_bytree=0.8,
    reg_alpha=0.97,
    reg_lambda=1.0,
    subsample=0.68,
    verbose=-1
)
lgbm_model.fit(X_train_resampled, y_train_resampled)

# Festgelegte optimale thresholds, auch diese davor ermittelt
best_xgb_thresh = 0.588
best_lgbm_thresh = 0.573

# Funktion zur Bewertung der Modelle mit optionalem Threshold
def evaluate_model(name, model, X_eval, y_eval, threshold=None):
    if threshold is None:
        y_pred = model.predict(X_eval)
    else:
        probs = model.predict_proba(X_eval)[:, 1]
        y_pred = (probs >= threshold).astype(int)

    print(f"\n{name} Klassifikationsbericht:")
    print(classification_report(y_eval, y_pred))

    return {
        "Model": name,
        "Accuracy": accuracy_score(y_eval, y_pred),
        "Classification Error": 1 - accuracy_score(y_eval, y_pred),
        "Recall": recall_score(y_eval, y_pred),
        "Precision": precision_score(y_eval, y_pred),
        "F1-Score": f1_score(y_eval, y_pred)
    }

results = []

# Baseline trainieren und evaluieren
baseline.fit(X_train, y_train)
results.append(evaluate_model("Baseline", baseline, X_test, y_test))

# Logistic Regression trainieren und evaluieren
logreg.fit(X_train, y_train)
results.append(evaluate_model("Logistic Regression", logreg, X_test, y_test))

# Decision Tree trainieren und evaluieren
tree.fit(X_train, y_train)
results.append(evaluate_model("Decision Tree", tree, X_test, y_test))

# Optimierten Random Forest evaluieren
results.append(evaluate_model("Random Forest", best_rf, X_test, y_test))

# XGBoost mit festem Schwellenwert evaluieren
results.append(evaluate_model("XGBoost", xgb_model, X_test, y_test, threshold=best_xgb_thresh))

# LightGBM mit festem Schwellenwert evaluieren
results.append(evaluate_model("LightGBM", lgbm_model, X_test, y_test, threshold=best_lgbm_thresh))

# Stacking-Modell trainieren
stacking_estimators = [
    ('logreg', logreg),
    ('tree', tree),
    ('rf', best_rf),
    ('xgb', xgb_model),
    ('lgbm', lgbm_model)
]
final_estimator = LogisticRegression(max_iter=1000)
stacking_clf = StackingClassifier(
    estimators=stacking_estimators,
    final_estimator=final_estimator,
    passthrough=False,
    n_jobs=-1
)
stacking_clf.fit(X_train, y_train)
results.append(evaluate_model("Stacking Classifier", stacking_clf, X_test, y_test))

# Zusammenfassung aller Modelle als DataFrame anzeigen
scores_df = pd.DataFrame(results)
print("\nZusammenfassung aller Modelle:")
print(scores_df)

# Modelle speichern
joblib.dump(baseline, "baseline_model.pkl")
joblib.dump(logreg, "logreg_model.pkl")
joblib.dump(tree, "tree_model.pkl")
joblib.dump(best_rf, "best_rf_model.pkl")
joblib.dump(xgb_model, "xgb_model.pkl")
joblib.dump(lgbm_model, "lgbm_model.pkl")
joblib.dump(stacking_clf, "stacking_model.pkl")